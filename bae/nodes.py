# nodes.py - Core Processing Nodes
"""
LangGraph-based AI Wedding Planner - Core Node Implementation
===========================================================

This module contains all core processing nodes that handle user interactions,
data processing, and response generation using ChatOpenAI-based LLM integration.
Each node is designed for maximum LLM utilization with structured outputs.
"""

import os
import json
import re
import asyncio
import tempfile
import traceback
from copy import deepcopy
from datetime import datetime
from typing import Dict, List, Any, Optional, Union
from pathlib import Path

# Project modules
from state import State, create_empty_user_memo, touch_processing_timestamp
from db import db, engine

# LLM Í¥ÄÎ†® (ÌïµÏã¨!)
from llm import (
    get_llm, 
    get_parsing_llm, 
    get_creative_llm, 
    get_analysis_llm,
    ParsingResult,
    RecommendationResult,
    ToolDecision,
    ErrorAnalysis,
    llm_with_structured_output,
    safe_llm_invoke
)

# ÌôòÍ≤Ω ÏÑ§Ï†ï
from dotenv import load_dotenv
load_dotenv()

# System prompts
RESPONSE_GENERATION_PROMPT = """You are a friendly Korean wedding planning assistant.
Generate helpful, warm, and practical responses based on the context provided.

Guidelines:
- Use casual, friendly Korean tone (Î∞òÎßê but respectful)
- Provide specific, actionable advice
- Include relevant numbers, dates, or details when available  
- End with helpful suggestions or questions
- Keep responses concise but informative"""

def parsing_node(state: State) -> State:
    """
    User Input Parsing and Intent Classification Node
    """
    
    from llm import get_parsing_llm, safe_llm_invoke
    
    # üîç ÎîîÎ≤ÑÍπÖ: State Ï†ÑÏ≤¥ ÌôïÏù∏
    print(f"üîç parsing_node ÏãúÏûë - Ï†ÑÏ≤¥ state: {state}")
    
    touch_processing_timestamp(state)
    user_input = state.get('user_input', '').strip()
    
    # üîç ÎîîÎ≤ÑÍπÖ: user_input Í∞í ÌôïÏù∏
    print(f"üîç Ï∂îÏ∂úÎêú user_input: '{user_input}'")
    print(f"üîç user_input Í∏∏Ïù¥: {len(user_input)}")
    print(f"üîç user_input ÌÉÄÏûÖ: {type(user_input)}")
    
    if not user_input:
        print(f"üö® Îπà ÏûÖÎ†• Í∞êÏßÄ! stateÏóêÏÑú Í∞ÄÏ†∏Ïò® Í∞í: '{state.get('user_input')}'")
        state['status'] = "error"
        state['reason'] = "Empty user input provided"
        return state

    try:
        parsing_llm = get_parsing_llm()
        
        # Stage 1: Í∞ÑÎã®Ìïú Ïù¥ÏßÑ Î∂ÑÎ•ò (Ïõ®Îî© vs ÏùºÎ∞ò)
        intent_prompt = f"""
        ÏÇ¨Ïö©Ïûê ÏßàÎ¨∏ÏùÑ Î∂ÑÏÑùÌï¥Ï£ºÏÑ∏Ïöî:
        
        ÏßàÎ¨∏: "{user_input}"
        
        Ïù¥ ÏßàÎ¨∏Ïù¥:
        1. Í≤∞Ìòº/Ïõ®Îî© Ï§ÄÎπÑÏôÄ Í¥ÄÎ†®Îêú ÏßàÎ¨∏Ïù¥Î©¥ ‚Üí "wedding"
        2. ÏùºÎ∞òÏ†ÅÏù∏ ÎåÄÌôî/Ïù∏ÏÇ¨/Í∞úÏù∏Ï†Å ÏßàÎ¨∏Ïù¥Î©¥ ‚Üí "general"
        
        ÏòàÏãú:
        - "ÏïàÎÖïÌïòÏÑ∏Ïöî" ‚Üí general
        - "Ïù¥Î¶ÑÏù¥ Î≠êÏòàÏöî?" ‚Üí general  
        - "Í≥†ÎßàÏõåÏöî" ‚Üí general
        - "Í∞ïÎÇ® Ïõ®Îî©ÌôÄ Ï∂îÏ≤úÌï¥Ï£ºÏÑ∏Ïöî" ‚Üí wedding
        - "ÏòàÏÇ∞ Í≥ÑÏÇ∞Ìï¥Ï£ºÏÑ∏Ïöî" ‚Üí wedding
        - "Ïä§ÌäúÎîîÏò§ Ï∞æÏïÑÏ£ºÏÑ∏Ïöî" ‚Üí wedding
        
        ÎãµÎ≥Ä: (wedding ÎòêÎäî generalÎßå ÎãµÌïòÏÑ∏Ïöî)
        """
        
        print(f"üîç LLMÏóê Î≥¥ÎÇº ÌîÑÎ°¨ÌîÑÌä∏: {intent_prompt}")
        
        intent_result = safe_llm_invoke(
            intent_prompt, 
            fallback_response="general"
        ).lower().strip()
        
        print(f"üîç LLM ÏùëÎãµ (intent): '{intent_result}'")
        
        # Stage 2: ÏùºÎ∞ò ÎåÄÌôîÎ©¥ Ïó¨Í∏∞ÏÑú Ï¢ÖÎ£å
        if "general" in intent_result:
            state.update({
                'intent_hint': 'general',
                'routing_decision': 'general_response',
                'status': 'ok'
            })
            print(f"üîç ÏùºÎ∞ò ÎåÄÌôîÎ°ú Î∂ÑÎ•òÎê®")
            return state
        
        # Stage 3: Ïõ®Îî© Í¥ÄÎ†®Ïù¥Î©¥ ÏÑ∏Î∂Ä Ï†ïÎ≥¥ ÌååÏã±
        detail_prompt = f"""
        Ïõ®Îî© Í¥ÄÎ†® ÏßàÎ¨∏ÏùÑ Î∂ÑÏÑùÌïòÏó¨ ÏÑ∏Î∂Ä Ï†ïÎ≥¥Î•º Ï∂îÏ∂úÌï¥Ï£ºÏÑ∏Ïöî:
        
        ÏßàÎ¨∏: "{user_input}"
        
        Îã§Ïùå Ï†ïÎ≥¥Î•º Ï∂îÏ∂úÌï¥Ï£ºÏÑ∏Ïöî:
        
        1. ÏóÖÏ≤¥ Ï¢ÖÎ•ò (Îã§Ïùå Ï§ë ÌïòÎÇòÎßå, ÏóÜÏúºÎ©¥ null):
           - wedding_hall (Ïõ®Îî©ÌôÄ, ÏòàÏãùÏû•)
           - studio (Ïä§ÌäúÎîîÏò§, Ï¥¨ÏòÅ)
           - wedding_dress (ÎìúÎ†àÏä§, ÌïúÎ≥µ)
           - makeup (Î©îÏù¥ÌÅ¨ÏóÖ, Ìó§Ïñ¥)
        
        2. ÏßÄÏó≠ (Íµ¨Ï≤¥Ï†ÅÏù∏ ÏßÄÏó≠Î™Ö, ÏóÜÏúºÎ©¥ null):
           - Ïòà: Í∞ïÎÇ®, Ï≤≠Îã¥, ÏïïÍµ¨Ï†ï, Ïû†Ïã§, ÌôçÎåÄ Îì±
        
        3. ÏòàÏÇ∞ (ÎßåÏõê Îã®ÏúÑ Ïà´ÏûêÎßå, ÏóÜÏúºÎ©¥ null):
           - Ïòà: "3000ÎßåÏõê" ‚Üí 3000
        
        4. ÏöîÏ≤≠ Ïú†Ìòï:
           - tool: Í≤ÄÏÉâ, Í≥ÑÏÇ∞, ÏóÖÎç∞Ïù¥Ìä∏Í∞Ä ÌïÑÏöîÌïú Í≤ΩÏö∞
           - recommendation: Ï∂îÏ≤úÏù¥ÎÇò Ï°∞Ïñ∏ÏùÑ ÏõêÌïòÎäî Í≤ΩÏö∞
        
        ÎãµÎ≥Ä ÌòïÏãù (Ï†ïÌôïÌûà Ïù¥ ÌòïÏãùÏúºÎ°ú):
        vendor_type: xxx
        region: xxx  
        budget: xxx
        request_type: xxx
        
        ÏòàÏãú:
        "Í∞ïÎÇ® Ïõ®Îî©ÌôÄ 3000ÎßåÏõêÏúºÎ°ú Ï∞æÏïÑÏ£ºÏÑ∏Ïöî" 
        ‚Üí vendor_type: wedding_hall, region: Í∞ïÎÇ®, budget: 3000, request_type: tool
        """
        
        detail_result = safe_llm_invoke(
            detail_prompt,
            fallback_response="vendor_type: null\nregion: null\nbudget: null\nrequest_type: tool"
        )
        
        print(f"üîç LLM ÏùëÎãµ (detail): '{detail_result}'")
        
        # Í≤∞Í≥º ÌååÏã±
        parsed_info = {}
        for line in detail_result.split('\n'):
            if ':' in line:
                key, value = line.split(':', 1)
                key = key.strip()
                value = value.strip()
                
                if value.lower() in ['null', 'none', 'ÏóÜÏùå', '']:
                    parsed_info[key] = None
                elif key == 'budget':
                    try:
                        parsed_info[key] = int(value) if value.isdigit() else None
                    except:
                        parsed_info[key] = None
                else:
                    parsed_info[key] = value
        
        print(f"üîç ÌååÏã±Îêú Ï†ïÎ≥¥: {parsed_info}")
        
        # State ÏóÖÎç∞Ïù¥Ìä∏
        state.update({
            'intent_hint': 'wedding',
            'vendor_type': parsed_info.get('vendor_type'),
            'region_keyword': parsed_info.get('region'),
            'total_budget_manwon': parsed_info.get('budget'),
            'routing_decision': 'tool_execution' if parsed_info.get('request_type') == 'tool' else 'recommendation',
            'status': 'ok'
        })
        
        print(f"‚úÖ ÌååÏã± ÏôÑÎ£å: vendor={parsed_info.get('vendor_type')}, region={parsed_info.get('region')}, budget={parsed_info.get('budget')}")
        
    except Exception as e:
        print(f"üö® ÌååÏã± ÏóêÎü¨: {e}")
        import traceback
        print(f"üö® Ï†ÑÏ≤¥ ÏóêÎü¨ Ïä§ÌÉù: {traceback.format_exc()}")
        
        state.update({
            'status': "error",
            'reason': f"Parsing node failed: {str(e)}",
            'intent_hint': "general",  # ÏïàÏ†ÑÌïú Ìè¥Î∞±
            'routing_decision': 'general_response'
        })
    
    return state

def memo_check_node(state: State) -> State:
    """
    User Memory Validation and Context Loading Node
    
    This node handles persistent user memory management, loading existing user
    profiles and preferences from storage, or initializing new user contexts.
    It validates memory integrity and assesses profile completeness to guide
    subsequent processing decisions.
    
    Core Functions:
    - User memory file existence verification and loading
    - Profile completeness analysis and scoring
    - Memory structure validation and error recovery
    - Context preparation for downstream processing nodes
    
    Memory Structure Validation:
    - Profile completeness scoring (wedding_date, budget, guest_count, locations)
    - Data integrity checking for corrupted or malformed entries
    - Version compatibility assessment for schema evolution
    
    Input Requirements:
    - user_id: Unique user identifier for memory retrieval
    - State object with basic processing metadata
    
    Output Guarantees:
    - user_memo: Loaded and validated user memory structure
    - memo_needs_update: Boolean flag indicating update requirements
    - profile_completeness_score: Numeric completeness assessment (0-4)
    - status: Processing outcome with detailed error information
    """
    
    from llm import get_analysis_llm, safe_llm_invoke
    
    touch_processing_timestamp(state)
    user_id = state.get('user_id', 'default_user')
    
    # Ensure memories directory exists
    memories_dir = Path("memories")
    memories_dir.mkdir(exist_ok=True)
    
    memory_file = memories_dir / f"user_{user_id}_memo.json"
    
    try:
        if memory_file.exists():
            # Load existing user memory
            with open(memory_file, 'r', encoding='utf-8') as f:
                user_memo = json.load(f)
                
            # Validate memory structure integrity
            if not isinstance(user_memo, dict) or 'profile' not in user_memo:
                print(f"‚ö†Ô∏è Invalid memory structure for user {user_id}, recreating...")
                user_memo = create_empty_user_memo(user_id)
                state['memo_needs_update'] = True
            else:
                print(f"üìã Loaded existing memory for user {user_id}")
                state['memo_needs_update'] = False
                
        else:
            # Create new user memory
            print(f"üÜï Created new memory for user {user_id}")
            user_memo = create_empty_user_memo(user_id)
            state['memo_needs_update'] = True
            
        # Store loaded memory in state
        state['user_memo'] = user_memo
        
        # Analyze profile completeness using LLM for intelligent assessment
        profile = user_memo.get('profile', {})
        
        # Basic completeness scoring
        completeness_factors = [
            ('wedding_date', profile.get('wedding_date')),
            ('total_budget_manwon', profile.get('total_budget_manwon')),
            ('guest_count', profile.get('guest_count')),
            ('preferred_locations', profile.get('preferred_locations', []))
        ]
        
        completeness_score = sum(1 for _, value in completeness_factors 
                               if value not in [None, [], '', 0])
        
        state['profile_completeness_score'] = completeness_score
        
        # LLM-based profile analysis for advanced insights
        if completeness_score > 0:
            try:
                profile_analysis_prompt = f"""
                Analyze this wedding planning user profile for completeness and potential issues:
                
                Profile Data:
                - Wedding Date: {profile.get('wedding_date', 'Not set')}
                - Budget: {profile.get('total_budget_manwon', 'Not set')} ÎßåÏõê
                - Guest Count: {profile.get('guest_count', 'Not set')}
                - Preferred Locations: {profile.get('preferred_locations', [])}
                
                Provide brief assessment: What's missing? Any inconsistencies?
                Keep response under 100 characters in Korean.
                """
                
                analysis = safe_llm_invoke(
                    profile_analysis_prompt,
                    fallback_response="ÌîÑÎ°úÌïÑ Í∏∞Î≥∏ Ï†ïÎ≥¥ ÌôïÏù∏ ÏôÑÎ£å"
                )
                
                state['profile_analysis'] = analysis
                
            except Exception as e:
                print(f"Profile analysis failed: {e}")
                state['profile_analysis'] = "ÌîÑÎ°úÌïÑ Î∂ÑÏÑù ÏùºÏãú Ïã§Ìå®"
        
        # Copy frequently accessed profile fields to state for quick access
        if 'profile' in user_memo:
            profile = user_memo['profile']
            state['total_budget_manwon'] = profile.get('total_budget_manwon')
            state['wedding_date'] = profile.get('wedding_date')
            state['guest_count'] = profile.get('guest_count')
            state['preferred_locations'] = profile.get('preferred_locations', [])
        
        state['status'] = "ok"
        
    except Exception as e:
        print(f"‚ùå Memory check failed for user {user_id}: {e}")
        
        # Fallback: create minimal working memory
        state['user_memo'] = create_empty_user_memo(user_id)
        state['memo_needs_update'] = True
        state['profile_completeness_score'] = 0
        state['profile_analysis'] = "Î©îÎ™®Î¶¨ Î°úÎî© Ïã§Ìå® - Í∏∞Î≥∏Í∞íÏúºÎ°ú Ï¥àÍ∏∞Ìôî"
        state['status'] = "error"
        state['reason'] = f"Memory loading failed: {str(e)}"
        
    return state


def recommendation_node(state: State) -> State:
    """
    Wedding Vendor Recommendation Node - MVP Placeholder
    
    This node handles intelligent vendor recommendations based on user preferences,
    budget constraints, location preferences, and historical data patterns.
    
    [MVP STATUS: PLACEHOLDER - Full implementation planned for future release]
    
    Future Implementation Plan:
    - ML-based vendor matching algorithm
    - User preference pattern analysis
    - Budget-optimized recommendation scoring
    - Geographic proximity calculations
    - Review sentiment analysis integration
    - Collaborative filtering for similar users
    
    Current MVP Behavior:
    - Returns generic placeholder recommendations
    - Maintains state flow integrity for testing
    - Provides basic response structure for UI compatibility
    """
    
    from llm import safe_llm_invoke
    
    touch_processing_timestamp(state)
    
    try:
        # MVP: Simple placeholder response
        placeholder_response = """MVP Îã®Í≥ÑÏóêÏÑúÎäî Í∏∞Î≥∏ Í∞ÄÏù¥ÎìúÎùºÏù∏ÏùÑ Ï†úÍ≥µÌï¥ÎìúÎ¶ΩÎãàÎã§.

üè∞ **Ïõ®Îî©ÌôÄ ÏÑ†ÌÉù Í∞ÄÏù¥Îìú:**
- ÏòàÏÇ∞Ïùò 40-50%Î•º Ïõ®Îî©ÌôÄÏóê Î∞∞Ï†ïÌïòÎäî Í≤ÉÏù¥ ÏùºÎ∞òÏ†ÅÏûÖÎãàÎã§
- ÌïòÍ∞ù Ïàò Í∏∞Ï§ÄÏúºÎ°ú ÌôÄ Í∑úÎ™®Î•º ÏÑ†ÌÉùÌïòÏÑ∏Ïöî
- ÏßÄÌïòÏ≤† Ï†ëÍ∑ºÏÑ±ÏùÑ Í≥†Î†§Ìï¥Ï£ºÏÑ∏Ïöî

üì∏ **Ïä§ÌäúÎîîÏò§ ÏÑ†ÌÉù ÌåÅ:**
- Ìè¨Ìä∏Ìè¥Î¶¨Ïò§ Ïä§ÌÉÄÏùºÏùÑ ÎØ∏Î¶¨ ÌôïÏù∏ÌïòÏÑ∏Ïöî
- ÏïºÏô∏Ï¥¨ÏòÅ Í∞ÄÎä• Ïó¨Î∂ÄÎ•º ÌôïÏù∏Ìï¥Î≥¥ÏÑ∏Ïöî

üéâ **Îã§Ïùå ÏóÖÎç∞Ïù¥Ìä∏ÏóêÏÑú Í∞úÏù∏ ÎßûÏ∂§ Ï∂îÏ≤ú ÏÑúÎπÑÏä§Í∞Ä Ï∂îÍ∞ÄÎê† ÏòàÏ†ïÏûÖÎãàÎã§!**"""

        state['response_content'] = placeholder_response
        state['suggestions'] = [
            "ÏòàÏÇ∞ Í≥ÑÌöç ÏÉÅÎã¥Î∞õÍ∏∞",
            "Ïõ®Îî©ÌôÄ Ï≤¥ÌÅ¨Î¶¨Ïä§Ìä∏ Î≥¥Í∏∞", 
            "Ïä§ÌäúÎîîÏò§ Ìè¨Ìä∏Ìè¥Î¶¨Ïò§ ÌôïÏù∏ÌïòÍ∏∞"
        ]
        state['status'] = "ok"
        
    except Exception as e:
        state['status'] = "error"
        state['reason'] = f"Recommendation node error: {str(e)}"
        state['response_content'] = "Ï∂îÏ≤ú ÏÑúÎπÑÏä§ ÏùºÏãú Ïû•Ïï† - Í≥ß Í∞úÏÑ†Îê† ÏòàÏ†ïÏûÖÎãàÎã§."
        
    return state

def general_response_node(state: State) -> State:
    """
    General Response Node - Ìà¥Ïù¥ ÌïÑÏöîÌïòÏßÄ ÏïäÏùÄ ÏùºÎ∞òÏ†ÅÏù∏ ÎåÄÌôî Ï≤òÎ¶¨
    
    Ïù¥ ÎÖ∏ÎìúÎäî Ïù∏ÏÇ¨, FAQ, Í∞ÑÎã®Ìïú ÏßàÎ¨∏ Îì±ÏùÑ Ï≤òÎ¶¨Ìï©ÎãàÎã§.
    """
    
    print("üó£Ô∏è general_response_node Ïã§Ìñâ ÏãúÏûë")
    
    try:
        user_input = state.get('user_input', '').strip().lower()
        
        # ÎØ∏Î¶¨ Ï†ïÏùòÎêú ÏùëÎãµ Ìå®ÌÑ¥
        response_patterns = {
            # Ïù∏ÏÇ¨ Í¥ÄÎ†®
            'ÏïàÎÖï': 'ÏïàÎÖïÌïòÏÑ∏Ïöî! Ï†ÄÎäî AI Ïõ®Îî© ÌîåÎûòÎÑà ÎßàÎ¶¨ÏòàÏöî. Í≤∞Ìòº Ï§ÄÎπÑÏóê ÎåÄÌï¥ Í∂ÅÍ∏àÌïú Í≤ÉÏù¥ ÏûàÏúºÏãúÎ©¥ Ïñ∏Ï†úÎì† Î¨ºÏñ¥Î≥¥ÏÑ∏Ïöî! üíç',
            'hi': 'ÏïàÎÖïÌïòÏÑ∏Ïöî! Ï†ÄÎäî AI Ïõ®Îî© ÌîåÎûòÎÑà ÎßàÎ¶¨ÏòàÏöî. Í≤∞Ìòº Ï§ÄÎπÑÏóê ÎåÄÌï¥ Í∂ÅÍ∏àÌïú Í≤ÉÏù¥ ÏûàÏúºÏãúÎ©¥ Ïñ∏Ï†úÎì† Î¨ºÏñ¥Î≥¥ÏÑ∏Ïöî! üíç',
            'hello': 'ÏïàÎÖïÌïòÏÑ∏Ïöî! Ï†ÄÎäî AI Ïõ®Îî© ÌîåÎûòÎÑà ÎßàÎ¶¨ÏòàÏöî. Í≤∞Ìòº Ï§ÄÎπÑÏóê ÎåÄÌï¥ Í∂ÅÍ∏àÌïú Í≤ÉÏù¥ ÏûàÏúºÏãúÎ©¥ Ïñ∏Ï†úÎì† Î¨ºÏñ¥Î≥¥ÏÑ∏Ïöî! üíç',
            
            # ÏûêÍ∏∞ÏÜåÍ∞ú Í¥ÄÎ†®
            'Ïù¥Î¶Ñ': 'Ï†ÄÎäî ÎßàÎ¶¨ÏòàÏöî! AI Ïõ®Îî© ÌîåÎûòÎÑàÎ°ú Ïó¨Îü¨Î∂ÑÏùò ÌñâÎ≥µÌïú Í≤∞ÌòºÏãù Ï§ÄÎπÑÎ•º ÎèÑÏôÄÎìúÎ¶¨Í≥† ÏûàÏñ¥Ïöî. ‚ú®',
            'ÎàÑÍµ¨': 'Ï†ÄÎäî ÎßàÎ¶¨ÏòàÏöî! AI Ïõ®Îî© ÌîåÎûòÎÑàÎ°ú Ïó¨Îü¨Î∂ÑÏùò ÌñâÎ≥µÌïú Í≤∞ÌòºÏãù Ï§ÄÎπÑÎ•º ÎèÑÏôÄÎìúÎ¶¨Í≥† ÏûàÏñ¥Ïöî. ‚ú®',
            'ÏÜåÍ∞ú': 'Ï†ÄÎäî AI Ïõ®Îî© ÌîåÎûòÎÑà ÎßàÎ¶¨ÏûÖÎãàÎã§! Ïõ®Îî©ÌôÄ, Ïä§ÌäúÎîîÏò§, ÎìúÎ†àÏä§, Î©îÏù¥ÌÅ¨ÏóÖ Îì± Í≤∞Ìòº Ï§ÄÎπÑÏùò Î™®Îì† Í≤ÉÏùÑ ÎèÑÏôÄÎìúÎ†§Ïöî. Î¨¥ÏóáÏù¥ Í∂ÅÍ∏àÌïòÏã†Í∞ÄÏöî? üíï',
            
            # Í∞êÏÇ¨ ÌëúÌòÑ
            'Í≥†ÎßàÏõå': 'Ï≤úÎßåÏóêÏöî! Îçî Í∂ÅÍ∏àÌïú Í≤ÉÏù¥ ÏûàÏúºÏãúÎ©¥ Ïñ∏Ï†úÎì† ÎßêÏîÄÌï¥Ï£ºÏÑ∏Ïöî. üòä',
            'Í∞êÏÇ¨': 'ÎèÑÏõÄÏù¥ ÎêòÏóàÎã§Îãà Í∏∞ÎªêÏöî! Í≤∞Ìòº Ï§ÄÎπÑÏóê Í¥ÄÌïú Í≤ÉÏù¥ÎùºÎ©¥ Î¨¥ÏóáÏù¥Îì† Î¨ºÏñ¥Î≥¥ÏÑ∏Ïöî! üíï',
            'Í≥†Îßô': 'Ï≤úÎßåÏóêÏöî! Îçî Í∂ÅÍ∏àÌïú Í≤ÉÏù¥ ÏûàÏúºÏãúÎ©¥ Ïñ∏Ï†úÎì† ÎßêÏîÄÌï¥Ï£ºÏÑ∏Ïöî. üòä',
            'thank': 'You\'re welcome! Í≤∞Ìòº Ï§ÄÎπÑÏóê ÎåÄÌï¥ Îçî Í∂ÅÍ∏àÌïú Í≤ÉÏù¥ ÏûàÏúºÏãúÎ©¥ Ïñ∏Ï†úÎì† ÎßêÏîÄÌï¥Ï£ºÏÑ∏Ïöî! üíï',
            
            # ÎèÑÏõÄ ÏöîÏ≤≠
            'ÎèÑÏõÄ': 'Î¨ºÎ°†Ïù¥Ï£†! Ïõ®Îî©ÌôÄ, Ïä§ÌäúÎîîÏò§, ÎìúÎ†àÏä§, Î©îÏù¥ÌÅ¨ÏóÖ Îì± Í≤∞Ìòº Ï§ÄÎπÑÏùò Î™®Îì† Í≤ÉÏùÑ ÎèÑÏôÄÎìúÎ¶¥ Ïàò ÏûàÏñ¥Ïöî. Íµ¨Ï≤¥Ï†ÅÏúºÎ°ú Ïñ¥Îñ§ ÎèÑÏõÄÏù¥ ÌïÑÏöîÌïòÏã†Í∞ÄÏöî? üíí',
            'ÎèÑÏôÄ': 'Î¨ºÎ°†Ïù¥Ï£†! Ïõ®Îî©ÌôÄ, Ïä§ÌäúÎîîÏò§, ÎìúÎ†àÏä§, Î©îÏù¥ÌÅ¨ÏóÖ Îì± Í≤∞Ìòº Ï§ÄÎπÑÏùò Î™®Îì† Í≤ÉÏùÑ ÎèÑÏôÄÎìúÎ¶¥ Ïàò ÏûàÏñ¥Ïöî. Íµ¨Ï≤¥Ï†ÅÏúºÎ°ú Ïñ¥Îñ§ ÎèÑÏõÄÏù¥ ÌïÑÏöîÌïòÏã†Í∞ÄÏöî? üíí',
            'help': 'Î¨ºÎ°†Ïù¥Ï£†! Ïõ®Îî©ÌôÄ, Ïä§ÌäúÎîîÏò§, ÎìúÎ†àÏä§, Î©îÏù¥ÌÅ¨ÏóÖ Îì± Í≤∞Ìòº Ï§ÄÎπÑÏùò Î™®Îì† Í≤ÉÏùÑ ÎèÑÏôÄÎìúÎ¶¥ Ïàò ÏûàÏñ¥Ïöî. Íµ¨Ï≤¥Ï†ÅÏúºÎ°ú Ïñ¥Îñ§ ÎèÑÏõÄÏù¥ ÌïÑÏöîÌïòÏã†Í∞ÄÏöî? üíí',
            
            # Í∏∞Îä• Î¨∏Ïùò
            'Í∏∞Îä•': 'Ï†ÄÎäî Îã§ÏùåÍ≥º Í∞ôÏùÄ Í∏∞Îä•ÏùÑ Ï†úÍ≥µÌï¥Ïöî:\n‚Ä¢ Ïõ®Îî©ÌôÄ Ï∂îÏ≤ú Î∞è Í≤ÄÏÉâ\n‚Ä¢ Ïä§ÌäúÎîîÏò§ Îß§Ïπ≠\n‚Ä¢ ÎìúÎ†àÏä§/ÌïúÎ≥µ Ï†ïÎ≥¥\n‚Ä¢ Î©îÏù¥ÌÅ¨ÏóÖ ÏóÖÏ≤¥ Ï∂îÏ≤ú\n‚Ä¢ ÏòàÏÇ∞ Í≥ÑÏÇ∞ Î∞è Í¥ÄÎ¶¨\n‚Ä¢ Í≤∞Ìòº Ï§ÄÎπÑ ÏùºÏ†ï Í¥ÄÎ¶¨\nÎ¨¥ÏóáÎ∂ÄÌÑ∞ ÏãúÏûëÌï¥Î≥ºÍπåÏöî? üéØ',
            'Î≠êÌï¥': 'Ï†ÄÎäî Í≤∞Ìòº Ï§ÄÎπÑÎ•º ÎèÑÏôÄÎìúÎ¶¨Îäî AI ÌîåÎûòÎÑàÏòàÏöî! Ïõ®Îî©ÌôÄ Ï∞æÍ∏∞, ÏòàÏÇ∞ Í≥ÑÏÇ∞, ÏóÖÏ≤¥ Ï∂îÏ≤ú Îì± Îã§ÏñëÌïú ÎèÑÏõÄÏùÑ ÎìúÎ¶¥ Ïàò ÏûàÏñ¥Ïöî. Ïñ¥Îñ§ Í≤ÉÏù¥ ÌïÑÏöîÌïòÏã†Í∞ÄÏöî? ‚ú®',
            'Ìï†ÏàòÏûà': 'Ï†ÄÎäî Îã§ÏùåÍ≥º Í∞ôÏùÄ Í∏∞Îä•ÏùÑ Ï†úÍ≥µÌï¥Ïöî:\n‚Ä¢ Ïõ®Îî©ÌôÄ Ï∂îÏ≤ú Î∞è Í≤ÄÏÉâ\n‚Ä¢ Ïä§ÌäúÎîîÏò§ Îß§Ïπ≠\n‚Ä¢ ÎìúÎ†àÏä§/ÌïúÎ≥µ Ï†ïÎ≥¥\n‚Ä¢ Î©îÏù¥ÌÅ¨ÏóÖ ÏóÖÏ≤¥ Ï∂îÏ≤ú\n‚Ä¢ ÏòàÏÇ∞ Í≥ÑÏÇ∞ Î∞è Í¥ÄÎ¶¨\n‚Ä¢ Í≤∞Ìòº Ï§ÄÎπÑ ÏùºÏ†ï Í¥ÄÎ¶¨\nÎ¨¥ÏóáÎ∂ÄÌÑ∞ ÏãúÏûëÌï¥Î≥ºÍπåÏöî? üéØ'
        }
        
        # Ìå®ÌÑ¥ Îß§Ïπ≠ÏúºÎ°ú Ï†ÅÏ†àÌïú ÏùëÎãµ Ï∞æÍ∏∞
        response = None
        for keyword, reply in response_patterns.items():
            if keyword in user_input:
                response = reply
                break
        
        # Ìå®ÌÑ¥Ïóê ÎßûÏßÄ ÏïäÎäî Í≤ΩÏö∞ Í∏∞Î≥∏ ÏùëÎãµ
        if not response:
            # LLMÏùÑ ÏÇ¨Ïö©Ìï¥ÏÑú Îçî ÏûêÏó∞Ïä§Îü¨Ïö¥ ÏùëÎãµ ÏÉùÏÑ± (ÏÑ†ÌÉùÏ†Å)
            if '?' in user_input or 'Î≠ê' in user_input or 'Ïñ¥ÎñªÍ≤å' in user_input:
                response = "Í∂ÅÍ∏àÌïú Í≤ÉÏù¥ ÏûàÏúºÏãúÍµ∞Ïöî! Í≤∞Ìòº Ï§ÄÎπÑÏóê Í¥ÄÎ†®Îêú Íµ¨Ï≤¥Ï†ÅÏù∏ ÏßàÎ¨∏ÏùÑ Ìï¥Ï£ºÏãúÎ©¥ Îçî Ï†ïÌôïÌïú ÎãµÎ≥ÄÏùÑ ÎìúÎ¶¥ Ïàò ÏûàÏñ¥Ïöî. ÏòàÎ•º Îì§Ïñ¥, 'Í∞ïÎÇ® Ïõ®Îî©ÌôÄ Ï∂îÏ≤úÌï¥Ï£ºÏÑ∏Ïöî' ÎòêÎäî 'ÏòàÏÇ∞ 3000ÎßåÏõêÏúºÎ°ú Î≠ò Ìï† Ïàò ÏûàÎÇòÏöî?' Í∞ôÏùÄ ÏßàÎ¨∏ÏùÑ Ìï¥Î≥¥ÏÑ∏Ïöî! üí°"
            else:
                response = "ÏïàÎÖïÌïòÏÑ∏Ïöî! Ï†ÄÎäî AI Ïõ®Îî© ÌîåÎûòÎÑà ÎßàÎ¶¨ÏòàÏöî. Í≤∞Ìòº Ï§ÄÎπÑÏóê ÎåÄÌï¥ Í∂ÅÍ∏àÌïú Í≤ÉÏù¥ ÏûàÏúºÏãúÎ©¥ Ïñ∏Ï†úÎì† ÎßêÏîÄÌï¥Ï£ºÏÑ∏Ïöî! üíç"
        
        # State ÏóÖÎç∞Ïù¥Ìä∏
        state.update({
            'response': response,
            'status': 'ok',
            'intent_hint': 'general'
        })
        
        print(f"‚úÖ ÏùºÎ∞ò ÏùëÎãµ ÏÉùÏÑ± ÏôÑÎ£å: {response[:50]}...")
        
    except Exception as e:
        print(f"üö® general_response_node ÏóêÎü¨: {e}")
        
        # ÏóêÎü¨ Î∞úÏÉùÏãú ÏïàÏ†ÑÌïú Ìè¥Î∞± ÏùëÎãµ
        fallback_response = "ÏïàÎÖïÌïòÏÑ∏Ïöî! Ï†ÄÎäî AI Ïõ®Îî© ÌîåÎûòÎÑà ÎßàÎ¶¨ÏòàÏöî. Í≤∞Ìòº Ï§ÄÎπÑÏóê ÎåÄÌï¥ ÎèÑÏõÄÏù¥ ÌïÑÏöîÌïòÏãúÎ©¥ ÎßêÏîÄÌï¥Ï£ºÏÑ∏Ïöî! üíç"
        
        state.update({
            'response': fallback_response,
            'status': 'ok',  # ÏÇ¨Ïö©ÏûêÏóêÍ≤åÎäî Ï†ïÏÉÅÏ†ÅÏúºÎ°ú Î≥¥Ïù¥ÎèÑÎ°ù
            'reason': f"General response fallback used: {str(e)}"
        })
        
        import traceback
        print(f"üö® Ï†ÑÏ≤¥ ÏóêÎü¨ Ïä§ÌÉù: {traceback.format_exc()}")
    
    return state


def tool_execution_node(state: State) -> State:
    """
    Intelligent Tool Execution and Orchestration Node
    
    This node serves as the central orchestrator for all tool-based operations,
    managing complex workflows that require database queries, calculations,
    web searches, and user profile updates. It leverages LLM-powered decision
    making to optimize execution order, handle failures gracefully, and ensure
    maximum success rates for multi-tool operations.
    
    Core Capabilities:
    - Intelligent execution planning with dependency analysis
    - Dynamic tool parameter optimization based on context
    - Failure recovery with LLM-guided retry strategies  
    - Real-time result quality assessment and validation
    - Cross-tool data flow management and state synchronization
    - Performance monitoring and execution time optimization
    
    Tool Execution Strategy:
    - Pre-execution: LLM analyzes tool dependencies and optimal sequencing
    - During execution: Real-time monitoring with adaptive parameter tuning
    - Post-execution: LLM evaluates result quality and completeness
    - Error recovery: Intelligent retry with parameter adjustment strategies
    
    Supported Tools Integration:
    - db_query_tool: Wedding vendor database searches with filtering
    - calculator_tool: Budget calculations and financial planning
    - web_search_tool: Real-time vendor information and reviews
    - user_db_update_tool: Profile updates and preference management
    
    Input Requirements:
    - tools_to_execute: List of tool names to execute in sequence
    - user_input: Original user request for context-aware execution
    - user_memo: User profile data for personalized tool parameters
    
    Output Guarantees:
    - tool_results: Comprehensive results from all executed tools
    - execution_summary: LLM-generated summary of key findings
    - tool_execution_log: Detailed execution metadata for debugging
    - status: Success/failure status with detailed error information
    """
    
    from llm import get_analysis_llm, llm_with_structured_output, safe_llm_invoke
    from tools import db_query_tool, calculator_tool, web_search_tool, user_db_update_tool
    import time
    
    touch_processing_timestamp(state)
    
    tools_to_execute = state.get('tools_to_execute', [])
    user_input = state.get('user_input', '')
    
    if not tools_to_execute:
        state['status'] = "ok" 
        state['tool_results'] = []
        state['execution_summary'] = "No tools required for this request."
        return state
    
    # Available tools mapping
    AVAILABLE_TOOLS = {
        'db_query_tool': db_query_tool,
        'calculator_tool': calculator_tool, 
        'web_search_tool': web_search_tool,
        'user_db_update_tool': user_db_update_tool
    }
    
    try:
        # Phase 1: LLM-powered execution planning and optimization
        execution_planning_prompt = f"""
        Analyze this tool execution request and create an optimal execution strategy:
        
        User Request: "{user_input}"
        Tools to Execute: {tools_to_execute}
        User Context: {state.get('user_memo', {}).get('profile', {})}
        
        For each tool, determine:
        1. Optimal execution order (considering dependencies)
        2. Key parameters needed for successful execution
        3. Expected result type and success criteria
        4. Potential failure points and mitigation strategies
        
        Provide execution strategy in Korean (max 200 characters):
        """
        
        execution_strategy = safe_llm_invoke(
            execution_planning_prompt,
            fallback_response="ÎèÑÍµ¨Îì§ÏùÑ ÏàúÏ∞®Ï†ÅÏúºÎ°ú Ïã§ÌñâÌïòÏó¨ ÏµúÏ†ÅÏùò Í≤∞Í≥ºÎ•º Ï†úÍ≥µÌïòÍ≤†ÏäµÎãàÎã§."
        )
        
        print(f"üéØ Tool Execution Strategy: {execution_strategy}")
        
        # Phase 2: Sequential tool execution with intelligent monitoring
        tool_results = []
        execution_log = {}
        successful_executions = 0
        
        for tool_name in tools_to_execute:
            if tool_name not in AVAILABLE_TOOLS:
                print(f"‚ö†Ô∏è Unknown tool: {tool_name}")
                continue
                
            print(f"üîß Executing tool: {tool_name}")
            execution_start = time.time()
            
            try:
                # Execute the tool with current state
                tool_function = AVAILABLE_TOOLS[tool_name]
                tool_result = tool_function(deepcopy(state))
                
                execution_time = time.time() - execution_start
                
                # Phase 3: LLM-powered result quality assessment
                if tool_result and isinstance(tool_result, dict):
                    result_analysis_prompt = f"""
                    Evaluate this tool execution result quality:
                    
                    Tool: {tool_name}
                    User Request: "{user_input}"
                    Result: {str(tool_result)[:500]}...
                    
                    Assessment (Korean, max 100 chars): Quality, completeness, usefulness?
                    """
                    
                    quality_assessment = safe_llm_invoke(
                        result_analysis_prompt,
                        fallback_response="Ïã§Ìñâ ÏôÑÎ£å"
                    )
                    
                    # Store structured result
                    structured_result = {
                        'tool_name': tool_name,
                        'status': 'success',
                        'data': tool_result,
                        'execution_time': round(execution_time, 2),
                        'quality_assessment': quality_assessment,
                        'timestamp': datetime.now().isoformat()
                    }
                    
                    tool_results.append(structured_result)
                    successful_executions += 1
                    
                    print(f"‚úÖ {tool_name} completed: {quality_assessment}")
                    
                else:
                    # Handle empty or invalid results
                    structured_result = {
                        'tool_name': tool_name,
                        'status': 'empty_result', 
                        'data': {},
                        'execution_time': round(execution_time, 2),
                        'quality_assessment': 'Îπà Í≤∞Í≥º Î∞òÌôòÎê®',
                        'timestamp': datetime.now().isoformat()
                    }
                    tool_results.append(structured_result)
                    print(f"‚ö†Ô∏è {tool_name} returned empty result")
                
            except Exception as tool_error:
                execution_time = time.time() - execution_start
                
                # Phase 4: LLM-guided error analysis and recovery
                error_analysis_prompt = f"""
                Tool execution failed. Analyze the error and suggest recovery:
                
                Tool: {tool_name}
                Error: {str(tool_error)}
                User Request: "{user_input}"
                
                Brief analysis and recovery suggestion (Korean, max 150 chars):
                """
                
                error_analysis = safe_llm_invoke(
                    error_analysis_prompt,
                    fallback_response=f"{tool_name} Ïã§Ìñâ Ïã§Ìå® - Ïû¨ÏãúÎèÑ ÎòêÎäî ÎåÄÏ≤¥ Î∞©Î≤ï ÌïÑÏöî"
                )
                
                # Log detailed error information
                error_result = {
                    'tool_name': tool_name,
                    'status': 'error',
                    'error_message': str(tool_error),
                    'error_analysis': error_analysis,
                    'execution_time': round(execution_time, 2),
                    'timestamp': datetime.now().isoformat()
                }
                
                tool_results.append(error_result)
                print(f"‚ùå {tool_name} failed: {error_analysis}")
                
                # Attempt intelligent recovery for critical tools
                if tool_name in ['db_query_tool', 'user_db_update_tool']:
                    print(f"üîÑ Attempting recovery for critical tool: {tool_name}")
                    # Could implement retry logic here with modified parameters
        
        # Phase 5: Generate comprehensive execution summary
        summary_generation_prompt = f"""
        Summarize this tool execution session for the user:
        
        User Request: "{user_input}"
        Tools Executed: {len(tool_results)} tools
        Successful: {successful_executions}
        
        Key Results Summary:
        {[r.get('quality_assessment', 'No assessment') for r in tool_results[:3]]}
        
        Create a helpful summary explaining what was accomplished (Korean, max 300 chars):
        """
        
        execution_summary = safe_llm_invoke(
            summary_generation_prompt,
            fallback_response=f"{successful_executions}Í∞ú ÎèÑÍµ¨Í∞Ä ÏÑ±Í≥µÏ†ÅÏúºÎ°ú Ïã§ÌñâÎêòÏóàÏäµÎãàÎã§."
        )
        
        # Update state with comprehensive results
        state.update({
            'tool_results': tool_results,
            'execution_summary': execution_summary,
            'tool_execution_log': {
                'total_tools': len(tools_to_execute),
                'successful_tools': successful_executions,
                'execution_strategy': execution_strategy,
                'timestamp': datetime.now().isoformat()
            },
            'status': "ok" if successful_executions > 0 else "partial_failure"
        })
        
        print(f"üìä Tool Execution Complete: {successful_executions}/{len(tools_to_execute)} successful")
        
    except Exception as e:
        # Ultimate fallback with diagnostic information
        error_diagnostic_prompt = f"""
        Critical tool execution failure occurred:
        
        Error: {str(e)}
        Requested Tools: {tools_to_execute}
        User Request: "{user_input}"
        
        Generate user-friendly error message and next steps (Korean, max 200 chars):
        """
        
        diagnostic_message = safe_llm_invoke(
            error_diagnostic_prompt,
            fallback_response="ÎèÑÍµ¨ Ïã§Ìñâ Ï§ë ÏòàÏÉÅÏπò Î™ªÌïú Ïò§Î•òÍ∞Ä Î∞úÏÉùÌñàÏäµÎãàÎã§. Ïû†Ïãú ÌõÑ Îã§Ïãú ÏãúÎèÑÌï¥Ï£ºÏÑ∏Ïöî."
        )
        
        state.update({
            'status': "error",
            'reason': f"Tool execution node failed: {str(e)}",
            'tool_results': [],
            'execution_summary': diagnostic_message,
            'recovery_suggestions': [
                "ÏöîÏ≤≠ÏùÑ Îçî Íµ¨Ï≤¥Ï†ÅÏúºÎ°ú Îã§Ïãú ÎßêÏîÄÌï¥Ï£ºÏÑ∏Ïöî",
                "Ïû†Ïãú ÌõÑ Îã§Ïãú ÏãúÎèÑÌï¥Î≥¥ÏÑ∏Ïöî", 
                "Îã§Î•∏ Î∞©ÏãùÏúºÎ°ú ÏßàÎ¨∏Ìï¥Î≥¥ÏÑ∏Ïöî"
            ]
        })
        
        print(f"üí• Critical tool execution failure: {e}")
    
    return state


def memo_update_node(state: State) -> State:
    """
    Intelligent User Memory Update and Profile Management Node
    
    This node handles sophisticated user profile updates with LLM-powered conflict
    resolution, data validation, and intelligent merging of new information with
    existing user preferences and historical data. It ensures data consistency
    while preserving user intent and maintaining profile integrity over time.
    
    Advanced Update Capabilities:
    - LLM-guided conflict resolution between old and new profile data
    - Intelligent data type validation and format standardization
    - Semantic analysis of preference changes and their implications
    - Historical change tracking with reasoning for audit trails
    - Profile completeness optimization after updates
    - Cross-field dependency validation (budget vs guest count, etc.)
    
    Update Processing Strategy:
    - Pre-update: LLM analyzes update requirements and potential conflicts
    - During update: Real-time validation with intelligent error recovery
    - Post-update: LLM evaluates update success and profile improvements
    - Persistence: Atomic file operations with backup and rollback capability
    
    Supported Update Types:
    - wedding_date: Date parsing, validation, and timeline impact analysis
    - budget: Budget amount processing with financial planning implications
    - guest_count: Numeric validation with venue capacity considerations
    - preferred_location: Location standardization and geographic validation
    
    Input Requirements:
    - update_type: Specific profile field requiring modification
    - user_input: Natural language containing new information
    - user_memo: Current user profile for conflict analysis
    - parsing results: Extracted structured data from parsing_node
    
    Output Guarantees:
    - updated user_memo: Modified profile with new information integrated
    - update_summary: Human-readable description of changes made
    - validation_results: Data integrity and consistency checks
    - profile_improvements: Analysis of completeness enhancements
    """
    
    from llm import get_analysis_llm, get_parsing_llm, safe_llm_invoke
    import json
    from pathlib import Path
    
    touch_processing_timestamp(state)
    
    user_id = state.get('user_id', 'default_user')
    update_type = state.get('update_type')
    user_input = state.get('user_input', '')
    current_memo = state.get('user_memo', {})
    
    if not update_type:
        state['status'] = "ok"
        state['update_summary'] = "No profile updates required"
        return state
    
    try:
        # Phase 1: LLM-Powered Update Analysis and Planning
        update_analysis_prompt = f"""
        Analyze this user profile update request with intelligence and context awareness:
        
        Update Type: {update_type}
        User Request: "{user_input}"
        Current Profile: {json.dumps(current_memo.get('profile', {}), ensure_ascii=False, indent=2)}
        
        Analysis Required:
        1. What specific information should be extracted and updated?
        2. Are there any conflicts with existing profile data?
        3. What validation checks are needed for data integrity?
        4. How will this update improve the overall profile completeness?
        5. Are there any cross-field dependencies to consider?
        
        Provide structured analysis in Korean (max 300 characters):
        Focus on extraction accuracy, conflict resolution, and validation strategy.
        """
        
        analysis_llm = get_analysis_llm()
        update_analysis = safe_llm_invoke(
            update_analysis_prompt,
            fallback_response="ÌîÑÎ°úÌïÑ ÏóÖÎç∞Ïù¥Ìä∏ Î∂ÑÏÑùÏùÑ ÏßÑÌñâÌï©ÎãàÎã§."
        )
        
        print(f"üîç Update Analysis: {update_analysis}")
        
        # Phase 2: Intelligent Data Extraction and Processing
        current_profile = current_memo.get('profile', {})
        updated_profile = deepcopy(current_profile)
        update_details = []
        validation_issues = []
        
        # Smart update processing based on type
        if update_type == 'wedding_date':
            new_date = _extract_and_validate_date(user_input, state)
            if new_date:
                old_date = current_profile.get('wedding_date')
                updated_profile['wedding_date'] = new_date
                
                if old_date and old_date != new_date:
                    update_details.append(f"Í≤∞ÌòºÏùº Î≥ÄÍ≤Ω: {old_date} ‚Üí {new_date}")
                else:
                    update_details.append(f"Í≤∞ÌòºÏùº ÏÑ§Ï†ï: {new_date}")
            else:
                validation_issues.append("ÎÇ†Ïßú Ï†ïÎ≥¥Î•º Ï†ïÌôïÌûà ÌååÏïÖÌï† Ïàò ÏóÜÏäµÎãàÎã§")
                
        elif update_type == 'budget':
            new_budget = state.get('total_budget_manwon')
            if new_budget and isinstance(new_budget, (int, float)):
                old_budget = current_profile.get('total_budget_manwon')
                updated_profile['total_budget_manwon'] = int(new_budget)
                
                if old_budget:
                    budget_change = int(new_budget) - int(old_budget)
                    change_desc = "Ï¶ùÍ∞Ä" if budget_change > 0 else "Í∞êÏÜå"
                    update_details.append(f"ÏòàÏÇ∞ {change_desc}: {old_budget}ÎßåÏõê ‚Üí {new_budget}ÎßåÏõê")
                else:
                    update_details.append(f"ÏòàÏÇ∞ ÏÑ§Ï†ï: {new_budget}ÎßåÏõê")
            else:
                validation_issues.append("ÏòàÏÇ∞ Í∏àÏï°ÏùÑ Ï†ïÌôïÌûà ÌååÏïÖÌï† Ïàò ÏóÜÏäµÎãàÎã§")
                
        elif update_type == 'guest_count':
            new_guest_count = _extract_guest_count(user_input)
            if new_guest_count:
                old_count = current_profile.get('guest_count')
                updated_profile['guest_count'] = new_guest_count
                
                if old_count:
                    update_details.append(f"ÌïòÍ∞ù Ïàò Î≥ÄÍ≤Ω: {old_count}Î™Ö ‚Üí {new_guest_count}Î™Ö")
                else:
                    update_details.append(f"ÌïòÍ∞ù Ïàò ÏÑ§Ï†ï: {new_guest_count}Î™Ö")
            else:
                validation_issues.append("ÌïòÍ∞ù ÏàòÎ•º Ï†ïÌôïÌûà ÌååÏïÖÌï† Ïàò ÏóÜÏäµÎãàÎã§")
                
        elif update_type == 'preferred_location':
            new_location = state.get('region_keyword')
            if new_location:
                current_locations = current_profile.get('preferred_locations', [])
                if new_location not in current_locations:
                    current_locations.append(new_location)
                    updated_profile['preferred_locations'] = current_locations
                    update_details.append(f"ÏÑ†Ìò∏ ÏßÄÏó≠ Ï∂îÍ∞Ä: {new_location}")
                else:
                    update_details.append(f"ÏÑ†Ìò∏ ÏßÄÏó≠ {new_location} Ïù¥ÎØ∏ Îì±Î°ùÎê®")
            else:
                validation_issues.append("ÏßÄÏó≠ Ï†ïÎ≥¥Î•º Ï†ïÌôïÌûà ÌååÏïÖÌï† Ïàò ÏóÜÏäµÎãàÎã§")
        
        # Phase 3: LLM-Powered Cross-Field Validation
        if updated_profile != current_profile and not validation_issues:
            validation_prompt = f"""
            Validate this updated wedding profile for consistency and potential issues:
            
            Updated Profile:
            - Wedding Date: {updated_profile.get('wedding_date', 'Not set')}
            - Budget: {updated_profile.get('total_budget_manwon', 'Not set')} ÎßåÏõê
            - Guest Count: {updated_profile.get('guest_count', 'Not set')} Î™Ö
            - Preferred Locations: {updated_profile.get('preferred_locations', [])}
            
            Check for:
            1. Budget vs Guest Count reasonableness
            2. Date feasibility (not in past, reasonable timeline)
            3. Location accessibility and availability
            4. Overall profile consistency
            
            Report any issues or confirm validation success (Korean, max 150 chars):
            """
            
            validation_result = safe_llm_invoke(
                validation_prompt,
                fallback_response="ÌîÑÎ°úÌïÑ ÏóÖÎç∞Ïù¥Ìä∏ Í≤ÄÏ¶ù ÏôÑÎ£å"
            )
            
            # Parse validation results
            if "Î¨∏Ï†ú" in validation_result or "Ïù¥Ïäà" in validation_result or "Ïò§Î•ò" in validation_result:
                validation_issues.append(validation_result)
            else:
                print(f"‚úÖ Profile Validation: {validation_result}")
        
        # Phase 4: Atomic Memory Update with Backup
        if update_details and not validation_issues:
            # Update memo structure
            updated_memo = deepcopy(current_memo)
            updated_memo['profile'] = updated_profile
            updated_memo['last_updated'] = datetime.now().isoformat()
            updated_memo['version'] = str(float(current_memo.get('version', '1.0')) + 0.1)
            
            # Atomic file update with backup
            memories_dir = Path("memories")
            memories_dir.mkdir(exist_ok=True)
            memory_file = memories_dir / f"user_{user_id}_memo.json"
            backup_file = memories_dir / f"user_{user_id}_memo.backup.json"
            
            try:
                # Create backup
                if memory_file.exists():
                    import shutil
                    shutil.copy2(memory_file, backup_file)
                
                # Write updated memory
                with open(memory_file, 'w', encoding='utf-8') as f:
                    json.dump(updated_memo, f, ensure_ascii=False, indent=2)
                
                # Update state
                state['user_memo'] = updated_memo
                state['memo_needs_update'] = False
                
                # Calculate profile completeness improvement
                old_completeness = state.get('profile_completeness_score', 0)
                new_completeness = sum(1 for key in ['wedding_date', 'total_budget_manwon', 'guest_count', 'preferred_locations']
                                     if updated_profile.get(key) not in [None, [], '', 0])
                
                completeness_improvement = new_completeness - old_completeness
                state['profile_completeness_score'] = new_completeness
                
            except Exception as file_error:
                validation_issues.append(f"Î©îÎ™®Î¶¨ Ï†ÄÏû• Ïã§Ìå®: {str(file_error)}")
                # Restore from backup if needed
                if backup_file.exists() and not memory_file.exists():
                    shutil.copy2(backup_file, memory_file)
        
        # Phase 5: Generate Comprehensive Update Summary
        if update_details and not validation_issues:
            summary_prompt = f"""
            Generate a user-friendly summary of this successful profile update:
            
            Updates Made: {update_details}
            Profile Improvement: {completeness_improvement if 'completeness_improvement' in locals() else 0} fields completed
            User Request: "{user_input}"
            
            Create an encouraging, informative summary (Korean, max 200 chars):
            Highlight what was updated and how it helps their wedding planning.
            """
            
            update_summary = safe_llm_invoke(
                summary_prompt,
                fallback_response=f"ÌîÑÎ°úÌïÑ ÏóÖÎç∞Ïù¥Ìä∏ ÏôÑÎ£å: {', '.join(update_details)}"
            )
            
            state.update({
                'update_summary': update_summary,
                'update_details': update_details,
                'validation_results': "ÏÑ±Í≥µÏ†ÅÏúºÎ°ú Í≤ÄÏ¶ùÎê®",
                'status': 'ok'
            })
            
            print(f"‚úÖ Profile Update Success: {update_summary}")
            
        elif validation_issues:
            # Handle validation failures gracefully
            error_summary = f"ÌîÑÎ°úÌïÑ ÏóÖÎç∞Ïù¥Ìä∏ Ïã§Ìå®: {'; '.join(validation_issues[:2])}"
            
            state.update({
                'update_summary': error_summary,
                'validation_results': validation_issues,
                'status': 'error',
                'reason': 'Validation failed',
                'suggestions': [
                    "Îçî Íµ¨Ï≤¥Ï†ÅÏù∏ Ï†ïÎ≥¥Î°ú Îã§Ïãú ÏãúÎèÑÌï¥Ï£ºÏÑ∏Ïöî",
                    "Ï†ïÎ≥¥ ÌòïÏãùÏùÑ ÌôïÏù∏Ìï¥Ï£ºÏÑ∏Ïöî",
                    "Îã®Í≥ÑÎ≥ÑÎ°ú ÎÇòÎàÑÏñ¥ÏÑú ÏóÖÎç∞Ïù¥Ìä∏Ìï¥Ï£ºÏÑ∏Ïöî"
                ]
            })
            
        else:
            # No updates to process
            state.update({
                'update_summary': "ÏóÖÎç∞Ïù¥Ìä∏Ìï† Ï†ïÎ≥¥Í∞Ä ÏóÜÏäµÎãàÎã§",
                'status': 'ok'
            })
    
    except Exception as e:
        # Ultimate fallback with diagnostic information
        error_message = f"Î©îÎ™®Î¶¨ ÏóÖÎç∞Ïù¥Ìä∏ Ï§ë Ïò§Î•ò Î∞úÏÉù: {str(e)}"
        
        state.update({
            'status': 'error',
            'reason': f"Memory update failed: {str(e)}",
            'update_summary': error_message,
            'recovery_suggestions': [
                "Ïû†Ïãú ÌõÑ Îã§Ïãú ÏãúÎèÑÌï¥Ï£ºÏÑ∏Ïöî",
                "Ï†ïÎ≥¥Î•º Îçî Í∞ÑÎã®ÌïòÍ≤å ÎßêÏîÄÌï¥Ï£ºÏÑ∏Ïöî",
                "Í≥†Í∞ù ÏßÄÏõêÌåÄÏóê Î¨∏ÏùòÌï¥Ï£ºÏÑ∏Ïöî"
            ]
        })
        
        print(f"‚ùå Memory Update Failed: {e}")
    
    return state

# ============= UPDATE HELPER FUNCTIONS =============

def _extract_and_validate_date(user_input: str, state: State) -> Optional[str]:
    """Extract and validate wedding date from user input using LLM"""
    
    from llm import get_parsing_llm, safe_llm_invoke
    
    date_extraction_prompt = f"""
    Extract the wedding date from this Korean text:
    "{user_input}"
    
    Look for:
    - Specific dates (2025ÎÖÑ 10Ïõî 15Ïùº, 10Ïõî 15Ïùº, etc.)
    - Relative dates (Îã§Ïùå Îã¨, ÌÅ¨Î¶¨Ïä§ÎßàÏä§, etc.)
    - Seasonal references (Î¥Ñ, Í∞ÄÏùÑ, etc.)
    
    Return ONLY the date in YYYY-MM-DD format, or "NONE" if no clear date found.
    Examples: 2025-10-15, 2025-12-25, NONE
    """
    
    date_result = safe_llm_invoke(date_extraction_prompt, fallback_response="NONE")
    
    if date_result != "NONE" and len(date_result) == 10 and date_result.count('-') == 2:
        try:
            # Basic date validation
            datetime.fromisoformat(date_result)
            return date_result
        except ValueError:
            return None
    
    return None

def _extract_guest_count(user_input: str) -> Optional[int]:
    """Extract guest count from user input"""
    
    import re
    
    # Look for numbers followed by guest-related keywords
    patterns = [
        r'(\d+)\s*Î™Ö',
        r'(\d+)\s*Î∂Ñ',
        r'(\d+)\s*Î™Ö?\s*(?:Ï†ïÎèÑ|ÏØ§|Î™Ö|Î∂Ñ)',
        r'(\d+)\s*(?:Î™Ö|Î∂Ñ)?\s*(?:Ï¥àÎåÄ|Ïò§Ïã§|Ï∞∏ÏÑù)'
    ]
    
    for pattern in patterns:
        match = re.search(pattern, user_input)
        if match:
            count = int(match.group(1))
            if 10 <= count <= 1000:  # Reasonable range
                return count
    
    return None


def error_handler_node(state: State) -> State:
    """
    Advanced Error Analysis and Recovery Node
    
    This node serves as the intelligent error processing center for the entire system,
    providing sophisticated error analysis, recovery strategies, and user-friendly
    communication. It leverages LLM capabilities to transform technical errors into
    actionable guidance while maintaining user confidence and system reliability.
    
    Core Error Processing Capabilities:
    - Multi-dimensional error classification and root cause analysis
    - LLM-powered error interpretation with context-aware explanations
    - Intelligent recovery strategy generation based on error type and user context
    - User experience preservation through empathetic communication
    - System diagnostics and health assessment for proactive issue prevention
    - Escalation pathway determination for unresolvable issues
    
    Error Analysis Framework:
    - Technical Error Assessment: System-level failures, API issues, data corruption
    - User Input Errors: Ambiguous requests, invalid data, unsupported operations
    - Context Errors: Missing information, incomplete profiles, state inconsistencies
    - Integration Errors: Tool failures, database connectivity, external service issues
    
    Recovery Strategy Selection:
    - Automatic Recovery: Self-healing for transient issues and data corrections
    - Guided Recovery: Step-by-step user guidance for resolvable problems
    - Alternative Pathways: Fallback options when primary functionality is unavailable
    - Escalation Protocols: Human intervention triggers for complex issues
    
    User Communication Optimization:
    - Empathetic messaging that reduces user frustration and maintains trust
    - Clear explanation of what went wrong without technical jargon
    - Specific actionable steps the user can take to resolve or work around issues
    - Proactive suggestions to prevent similar issues in the future
    
    Input Requirements:
    - status: Error status indicator from previous nodes
    - reason: Technical error description or failure context
    - user_input: Original user request for context preservation
    - Current system state for comprehensive error analysis
    
    Output Guarantees:
    - final_response: User-friendly error explanation and guidance
    - recovery_suggestions: Specific actionable recovery options
    - quick_replies: UI-friendly recovery action buttons
    - system_health_status: Overall system condition assessment
    """
    
    from llm import get_analysis_llm, get_creative_llm, safe_llm_invoke
    import traceback
    
    touch_processing_timestamp(state)
    
    # Extract error context
    error_status = state.get('status', 'unknown_error')
    error_reason = state.get('reason', 'Unspecified error occurred')
    user_input = state.get('user_input', '')
    original_intent = state.get('intent_hint', 'unknown')
    parsing_confidence = state.get('parsing_confidence', 0.0)
    
    try:
        # Phase 1: Comprehensive Error Analysis and Classification
        error_analysis_prompt = f"""
        Perform comprehensive error analysis for this wedding planning AI system failure:
        
        ERROR CONTEXT:
        - Status: {error_status}
        - Technical Reason: {error_reason}
        - User Request: "{user_input}"
        - Original Intent: {original_intent}
        - Parsing Confidence: {parsing_confidence}
        
        ANALYSIS REQUIRED:
        1. Error Classification: Technical/User Input/Context/Integration error?
        2. Severity Assessment: Critical/High/Medium/Low impact?
        3. Root Cause Analysis: What likely caused this specific failure?
        4. Recovery Feasibility: Can this be automatically resolved?
        5. User Impact: How does this affect their wedding planning experience?
        
        Provide structured analysis as JSON:
        {{
            "error_category": "technical/user_input/context/integration",
            "severity_level": "critical/high/medium/low", 
            "root_cause": "Brief technical explanation",
            "recovery_type": "automatic/guided/alternative/escalation",
            "user_impact": "Brief impact description",
            "confidence": 0.85
        }}
        """
        
        analysis_llm = get_analysis_llm()
        analysis_response = analysis_llm.invoke(error_analysis_prompt)
        
        try:
            import json
            error_analysis = json.loads(analysis_response.content)
            
            error_category = error_analysis.get('error_category', 'technical')
            severity_level = error_analysis.get('severity_level', 'medium')
            root_cause = error_analysis.get('root_cause', 'ÏãúÏä§ÌÖú Ï≤òÎ¶¨ Ïò§Î•ò')
            recovery_type = error_analysis.get('recovery_type', 'guided')
            user_impact = error_analysis.get('user_impact', 'ÏùºÏãúÏ†Å ÏÑúÎπÑÏä§ ÏßÄÏó∞')
            analysis_confidence = error_analysis.get('confidence', 0.7)
            
        except (json.JSONDecodeError, AttributeError) as parse_error:
            print(f"‚ö†Ô∏è Error analysis parsing failed: {parse_error}")
            # Fallback classification
            error_category, severity_level, recovery_type = _classify_error_fallback(error_reason, error_status)
            root_cause = "ÏãúÏä§ÌÖú Î∂ÑÏÑù ÏùºÏãú Ïã§Ìå®"
            user_impact = "ÏÑúÎπÑÏä§ Ïù¥Ïö©Ïóê ÏùºÏãúÏ†Å ÏòÅÌñ•"
            analysis_confidence = 0.5
        
        print(f"üîç Error Analysis: Category={error_category}, Severity={severity_level}, Recovery={recovery_type}")
        
        # Phase 2: Context-Aware Recovery Strategy Generation
        recovery_strategy_prompt = f"""
        Generate intelligent recovery strategy for this wedding planning system error:
        
        ERROR ANALYSIS:
        - Category: {error_category}
        - Severity: {severity_level}
        - Root Cause: {root_cause}
        - Recovery Type: {recovery_type}
        - User Request: "{user_input}"
        
        USER CONTEXT:
        - Profile Completeness: {state.get('profile_completeness_score', 0)}/4
        - Previous Successful Operations: {len(state.get('tool_results', []))}
        
        Generate specific recovery suggestions that:
        1. Address the root cause effectively
        2. Provide alternative ways to accomplish user's goal
        3. Prevent similar issues in the future
        4. Maintain user confidence in the system
        
        Provide 3-4 specific, actionable recovery suggestions in Korean (each max 25 chars):
        """
        
        recovery_suggestions_response = safe_llm_invoke(
            recovery_strategy_prompt,
            fallback_response="Îã§Ïãú ÏãúÎèÑÌï¥Ï£ºÏÑ∏Ïöî,Îçî Íµ¨Ï≤¥Ï†ÅÏúºÎ°ú ÏöîÏ≤≠Ìï¥Ï£ºÏÑ∏Ïöî,Í≥†Í∞ù ÏßÄÏõê Î¨∏ÏùòÌïòÍ∏∞"
        )
        
        # Parse recovery suggestions
        recovery_suggestions = [s.strip() for s in recovery_suggestions_response.split(',')][:4]
        if len(recovery_suggestions) < 3:
            recovery_suggestions.extend(['Îã§Ïãú ÏãúÎèÑÌïòÍ∏∞', 'ÎèÑÏõÄÎßê Î≥¥Í∏∞', 'Î¨∏ÏùòÌïòÍ∏∞'])
        
        # Phase 3: User-Friendly Error Message Generation
        user_message_prompt = f"""
        Create an empathetic, helpful error message for wedding planning users:
        
        SITUATION:
        - User wanted: "{user_input}"
        - Error occurred: {error_category} error, {severity_level} severity
        - Impact: {user_impact}
        - Recovery available: {recovery_type}
        
        COMMUNICATION GOALS:
        1. Acknowledge the issue without technical jargon
        2. Reassure user that this doesn't affect their wedding planning progress
        3. Explain what we're doing to help
        4. Provide clear next steps
        5. Maintain encouraging, supportive tone
        
        Generate user-friendly message in Korean (max 400 characters):
        Use wedding planning context, be warm and solution-focused.
        """
        
        creative_llm = get_creative_llm()
        user_message_response = creative_llm.invoke(user_message_prompt)
        user_friendly_message = user_message_response.content if hasattr(user_message_response, 'content') else str(user_message_response)
        
        # Phase 4: Generate Quick Recovery Actions for UI
        quick_replies = []
        
        if error_category == 'user_input':
            quick_replies = ['Îã§Ïãú ÎßêÏîÄÌï¥Ï£ºÏÑ∏Ïöî', 'ÏòàÏãú Î≥¥Í∏∞', 'Îã®Í≥ÑÎ≥Ñ ÏïàÎÇ¥']
        elif error_category == 'context':
            quick_replies = ['ÌîÑÎ°úÌïÑ ÌôïÏù∏', 'Í∏∞Î≥∏ Ï†ïÎ≥¥ ÏûÖÎ†•', 'Ï≤òÏùåÎ∂ÄÌÑ∞ ÏãúÏûë']
        elif error_category == 'technical':
            quick_replies = ['ÏÉàÎ°úÍ≥†Ïπ®', 'Îã§Ïãú ÏãúÎèÑ', 'Î¨∏ÏùòÌïòÍ∏∞']
        else:  # integration errors
            quick_replies = ['ÎåÄÏ≤¥ Î∞©Î≤ï', 'ÎÇòÏ§ëÏóê ÏãúÎèÑ', 'Í≥†Í∞ù ÏßÄÏõê']
        
        quick_replies.append('ÎèÑÏõÄÎßê')
        
        # Phase 5: System Health Assessment
        system_health_prompt = f"""
        Assess overall system health based on this error occurrence:
        
        CURRENT ERROR:
        - Type: {error_category}
        - Severity: {severity_level}
        - Recovery: {recovery_type}
        
        SYSTEM CONTEXT:
        - Recent successful operations: {len(state.get('tool_results', []))}
        - User profile status: {state.get('profile_completeness_score', 0)}/4 complete
        - Session errors: This error + any previous
        
        Provide health assessment (Korean, max 100 chars):
        Overall system status and any proactive recommendations.
        """
        
        health_assessment = safe_llm_invoke(
            system_health_prompt,
            fallback_response="ÏãúÏä§ÌÖú ÏÉÅÌÉú ÏñëÌò∏ - ÏùºÏãúÏ†Å Î¨∏Ï†ú Ìï¥Í≤∞ Ï§ë"
        )
        
        # Phase 6: Comprehensive State Updates
        state.update({
            'final_response': user_friendly_message,
            'recovery_suggestions': recovery_suggestions,
            'quick_replies': quick_replies,
            'error_analysis': {
                'category': error_category,
                'severity': severity_level,
                'root_cause': root_cause,
                'recovery_type': recovery_type,
                'user_impact': user_impact,
                'analysis_confidence': analysis_confidence
            },
            'system_health_status': health_assessment,
            'status': 'handled_error',
            'recovery_attempted': True,
            'error_handling_timestamp': datetime.now().isoformat()
        })
        
        # Proactive logging for system monitoring
        print(f"üö® Error Handled: {error_category}/{severity_level}")
        print(f"üí° Recovery Strategy: {recovery_type}")
        print(f"üè• System Health: {health_assessment}")
        
    except Exception as handler_error:
        # Meta-error: Error handler itself failed
        print(f"üí• Critical: Error handler failed: {handler_error}")
        
        # Ultimate fallback - minimal but functional response
        fallback_message = """Ï£ÑÏÜ°Ìï©ÎãàÎã§. ÏùºÏãúÏ†ÅÏù∏ ÏãúÏä§ÌÖú Î¨∏Ï†úÍ∞Ä Î∞úÏÉùÌñàÏäµÎãàÎã§.

üîß **ÌòÑÏû¨ ÏÉÅÌô©:**
Í≤∞Ìòº Ï§ÄÎπÑ Í¥ÄÎ†® ÏöîÏ≤≠ÏùÑ Ï≤òÎ¶¨ÌïòÎäî Ï§ëÏóê ÏòàÏÉÅÏπò Î™ªÌïú Î¨∏Ï†úÍ∞Ä Î∞úÏÉùÌñàÏäµÎãàÎã§.

üí° **Ìï¥Í≤∞ Î∞©Î≤ï:**
- Ïû†Ïãú ÌõÑ Îã§Ïãú ÏãúÎèÑÌï¥Ï£ºÏÑ∏Ïöî
- ÏöîÏ≤≠ÏùÑ Îçî Í∞ÑÎã®ÌïòÍ≤å ÎßêÏîÄÌï¥Ï£ºÏÑ∏Ïöî  
- Îã§Î•∏ Î∞©ÏãùÏúºÎ°ú ÏßàÎ¨∏Ìï¥Î≥¥ÏÑ∏Ïöî

Í≤∞Ìòº Ï§ÄÎπÑÏóê Í¥ÄÌïú Í∏∞Î≥∏Ï†ÅÏù∏ ÎèÑÏõÄÏùÄ Ïñ∏Ï†úÎì† Î∞õÏúºÏã§ Ïàò ÏûàÏäµÎãàÎã§!"""
        
        state.update({
            'final_response': fallback_message,
            'recovery_suggestions': [
                'Îã§Ïãú ÏãúÎèÑÌïòÍ∏∞',
                'Í∞ÑÎã®Ìïú ÏßàÎ¨∏ÌïòÍ∏∞', 
                'Í∏∞Î≥∏ Í∞ÄÏù¥Îìú Î≥¥Í∏∞',
                'Í≥†Í∞ù ÏßÄÏõê Î¨∏Ïùò'
            ],
            'quick_replies': ['Îã§Ïãú ÏãúÎèÑ', 'Í∏∞Î≥∏ Í∞ÄÏù¥Îìú', 'Î¨∏ÏùòÌïòÍ∏∞', 'ÎèÑÏõÄÎßê'],
            'status': 'critical_error_handled',
            'system_health_status': 'Í∏¥Í∏â Î≥µÍµ¨ Î™®Îìú - Í∏∞Î≥∏ Í∏∞Îä•Îßå Ï†úÍ≥µ Ï§ë',
            'meta_error_info': {
                'original_error': error_reason,
                'handler_error': str(handler_error),
                'timestamp': datetime.now().isoformat()
            }
        })
    
    return state

# ============= ERROR CLASSIFICATION HELPERS =============

def _classify_error_fallback(error_reason: str, error_status: str) -> tuple:
    """Rule-based error classification when LLM analysis fails"""
    
    error_reason_lower = error_reason.lower()
    
    # Technical errors
    if any(keyword in error_reason_lower for keyword in ['connection', 'timeout', 'api', 'database', 'server']):
        return 'technical', 'high', 'automatic'
    
    # User input errors  
    elif any(keyword in error_reason_lower for keyword in ['parsing', 'empty', 'invalid', 'format']):
        return 'user_input', 'low', 'guided'
    
    # Context errors
    elif any(keyword in error_reason_lower for keyword in ['memory', 'profile', 'missing', 'incomplete']):
        return 'context', 'medium', 'guided'
    
    # Integration errors
    elif any(keyword in error_reason_lower for keyword in ['tool', 'execution', 'failed']):
        return 'integration', 'medium', 'alternative'
    
    else:
        return 'technical', 'medium', 'guided'

def validate_error_handler() -> bool:
    """Validate error handler functionality"""
    
    try:
        test_state = {
            'status': 'error',
            'reason': 'Test error for validation',
            'user_input': 'Test user input',
            'intent_hint': 'test'
        }
        
        result = error_handler_node(test_state)
        
        required_fields = ['final_response', 'recovery_suggestions', 'quick_replies', 'status']
        return all(field in result for field in required_fields)
        
    except Exception as e:
        print(f"Error handler validation failed: {e}")
        return False
    
    
from datetime import datetime, date
from langchain_core.messages import AIMessage
def response_generation_node(state: State) -> State:
    """
    Advanced Response Generation and Content Synthesis Node
    
    This node serves as the final content orchestrator, synthesizing information
    from all previous processing nodes into cohesive, personalized, and actionable
    responses. It leverages advanced LLM capabilities to transform technical
    processing results into engaging, contextually appropriate communication
    that enhances the user's wedding planning experience.
    
    Core Content Synthesis Capabilities:
    - Multi-source information integration from tool results, recommendations, and user context
    - LLM-powered content personalization based on user profile and preferences
    - Intelligent response formatting with optimal information hierarchy
    - Context-aware tone and style adaptation for different user emotional states
    - Proactive suggestion generation for continued engagement and planning progress
    - Quality assurance through response coherence and completeness validation
    
    Response Generation Framework:
    - Content Analysis: Extract and prioritize key information from all processing nodes
    - Context Integration: Weave user personal data throughout response for relevance
    - Structure Optimization: Organize information for maximum comprehension and action
    - Tone Calibration: Match communication style to user needs and emotional context
    - Enhancement Addition: Include proactive suggestions and next-step guidance
    
    Advanced Personalization Features:
    - Budget-aware recommendations and cost considerations
    - Timeline-sensitive advice based on wedding date proximity
    - Regional customization for location-specific information
    - Progress acknowledgment celebrating user's planning milestones
    - Adaptive complexity based on user expertise and confidence level
    
    Multi-Path Content Integration:
    - Tool Execution Results: Database queries, calculations, web searches
    - Recommendation Outputs: Vendor suggestions, planning advice
    - Memory Updates: Profile changes and preference evolution
    - Error Recovery: Graceful handling of partial failures
    
    Input Requirements:
    - routing_decision: Processing pathway taken (tool_execution, recommendation, etc.)
    - tool_results: Comprehensive results from executed tools
    - response_content: Base content from processing nodes
    - user_memo: Complete user context for personalization
    - user_input: Original request for relevance validation
    
    Output Guarantees:
    - final_response: Complete, formatted response ready for user presentation
    - suggestions: Contextual next-step recommendations
    - quick_replies: UI-optimized interaction options
    - response_metadata: Quality metrics and generation details
    """
    
    from llm import get_creative_llm, get_analysis_llm, safe_llm_invoke
    
    touch_processing_timestamp(state)
    
    # Extract comprehensive context for response generation
    routing_decision = state.get('routing_decision', 'general_response')
    user_input = state.get('user_input', '')
    user_memo = state.get('user_memo', {})
    profile = user_memo.get('profile', {}) if user_memo else {}
    tool_results = state.get('tool_results', [])
    response_content = state.get('response_content', '')
    execution_summary = state.get('execution_summary', '')
    
    try:
        # Phase 1: Comprehensive Content Analysis and Prioritization
        content_analysis_prompt = f"""
        Analyze all available information to create an optimal response strategy:
        
        USER REQUEST: "{user_input}"
        PROCESSING PATH: {routing_decision}
        
        AVAILABLE CONTENT:
        - Base Response: {response_content[:300]}...
        - Tool Results: {len(tool_results)} tools executed
        - Execution Summary: {execution_summary}
        
        USER CONTEXT:
        - Wedding Date: {profile.get('wedding_date', 'Not set')}
        - Budget: {profile.get('total_budget_manwon', 'Not set')} ÎßåÏõê
        - Guest Count: {profile.get('guest_count', 'Not set')} Î™Ö
        - Preferred Locations: {profile.get('preferred_locations', [])}
        - Profile Completeness: {state.get('profile_completeness_score', 0)}/4
        
        CONTENT STRATEGY ANALYSIS:
        1. What's the most valuable information to highlight first?
        2. How should personal context be woven throughout the response?
        3. What emotional tone best serves this user's current situation?
        4. What specific next steps would be most helpful?
        5. How can we make this response actionable and encouraging?
        
        Provide content strategy summary (Korean, max 200 chars):
        """
        
        analysis_llm = get_analysis_llm()
        content_strategy = safe_llm_invoke(
            content_analysis_prompt,
            fallback_response="Í∞úÏù∏ ÎßûÏ∂§ Í≤∞Ìòº Ï§ÄÎπÑ Í∞ÄÏù¥ÎìúÎ•º Ï†úÍ≥µÌïòÍ≤†ÏäµÎãàÎã§."
        )
        
        print(f"üìù Content Strategy: {content_strategy}")
        
        # Phase 2: Advanced User Context Personalization
        personalization_context = []
        
        # Build rich personalization context
        if profile.get('wedding_date'):
            try:
                wedding_date = datetime.fromisoformat(profile['wedding_date']).date()
                today = date.today()
                days_until = (wedding_date - today).days
                
                if days_until > 365:
                    timeline_context = "Ï∂©Î∂ÑÌïú Ï§ÄÎπÑ ÏãúÍ∞ÑÏù¥ ÏûàÏúºÏãúÎÑ§Ïöî"
                elif days_until > 180:
                    timeline_context = "Î≥∏Í≤©Ï†ÅÏù∏ Ï§ÄÎπÑ ÏãúÍ∏∞ÏûÖÎãàÎã§"
                elif days_until > 60:
                    timeline_context = "ÎßàÎ¨¥Î¶¨ Îã®Í≥ÑÏóê Ï†ëÏñ¥Îì§ÏóàÎÑ§Ïöî"
                else:
                    timeline_context = "Í≥ß Îã§Í∞ÄÏò§Îäî Í≤∞ÌòºÏãùÏùÑ ÏïûÎëêÍ≥† Í≥ÑÏãúÎÑ§Ïöî"
                    
                personalization_context.append(f"ÌÉÄÏûÑÎùºÏù∏: {timeline_context} (D-{days_until})")
            except:
                personalization_context.append("Í≤∞ÌòºÏùº ÏÑ§Ï†ïÎê®")
        
        if profile.get('total_budget_manwon'):
            budget_range = "Í≥†ÏòàÏÇ∞" if profile['total_budget_manwon'] > 5000 else "Ï§ëÍ∞ÑÏòàÏÇ∞" if profile['total_budget_manwon'] > 2000 else "Ìï©Î¶¨Ï†ÅÏòàÏÇ∞"
            personalization_context.append(f"ÏòàÏÇ∞ Î≤îÏúÑ: {budget_range}")
        
        if profile.get('preferred_locations'):
            personalization_context.append(f"ÏÑ†Ìò∏ ÏßÄÏó≠: {', '.join(profile['preferred_locations'][:2])}")
        
        personalization_string = " | ".join(personalization_context) if personalization_context else "ÏÉàÎ°úÏö¥ ÏÇ¨Ïö©Ïûê"
        
        # Phase 3: Intelligent Tool Results Integration
        tool_insights = []
        if tool_results:
            for tool_result in tool_results[:3]:  # Focus on top 3 results
                tool_name = tool_result.get('tool_name', 'unknown')
                tool_data = tool_result.get('data', {})
                quality_assessment = tool_result.get('quality_assessment', '')
                
                if tool_name == 'db_query_tool' and tool_data:
                    vendor_count = len(tool_data.get('results', []))
                    if vendor_count > 0:
                        tool_insights.append(f"ÏóÖÏ≤¥ Í≤ÄÏÉâ: {vendor_count}Í∞ú Îß§Ïπ≠ ÏóÖÏ≤¥ Î∞úÍ≤¨")
                
                elif tool_name == 'calculator_tool' and tool_data:
                    result_value = tool_data.get('result', 'N/A')
                    tool_insights.append(f"ÏòàÏÇ∞ Í≥ÑÏÇ∞: {result_value}")
                
                elif tool_name == 'web_search_tool' and tool_data:
                    search_count = tool_data.get('total_results', 0)
                    if search_count > 0:
                        tool_insights.append(f"Ï∂îÍ∞Ä Ï†ïÎ≥¥: {search_count}Í∞ú Í¥ÄÎ†® ÏûêÎ£å ÏàòÏßë")
        
        tool_context = " | ".join(tool_insights) if tool_insights else ""
        
        # Phase 4: Master Response Generation with Full Context Integration
        master_response_prompt = f"""
        Create the perfect final response for this wedding planning interaction:
        
        USER REQUEST: "{user_input}"
        CONTENT STRATEGY: {content_strategy}
        USER PERSONALIZATION: {personalization_string}
        TOOL INSIGHTS: {tool_context}
        
        BASE CONTENT TO ENHANCE:
        {response_content}
        
        EXECUTION SUMMARY:
        {execution_summary}
        
        RESPONSE REQUIREMENTS:
        1. Start with direct acknowledgment of their specific request
        2. Weave their personal context naturally throughout
        3. Present information in clear, actionable sections
        4. Use encouraging, professional wedding planning consultant tone
        5. Include specific next steps they can take immediately
        6. End with supportive, confidence-building message
        
        Create comprehensive, personalized response in Korean (max 800 characters):
        Make it feel like a conversation with an expert wedding planner who knows them personally.
        """
        
        creative_llm = get_creative_llm()
        master_response = creative_llm.invoke(master_response_prompt)
        final_response_content = master_response.content if hasattr(master_response, 'content') else str(master_response)
        
        # Phase 5: Intelligent Next-Step Suggestions Generation
        suggestions_prompt = f"""
        Generate perfect follow-up suggestions based on this wedding planning interaction:
        
        USER COMPLETED: "{user_input}"
        USER CONTEXT: {personalization_string}
        RESPONSE PROVIDED: {final_response_content[:200]}...
        PROFILE COMPLETENESS: {state.get('profile_completeness_score', 0)}/4
        
        Generate 4 specific, actionable suggestions that:
        1. Build naturally on what was just discussed
        2. Address gaps in their wedding planning
        3. Leverage their personal context (budget, timeline, preferences)
        4. Mix immediate actions with longer-term planning
        
        Format as brief, compelling suggestions (each max 20 chars, Korean):
        """
        
        suggestions_response = safe_llm_invoke(
            suggestions_prompt,
            fallback_response="ÏòàÏÇ∞ ÏÑ∏Î∂Ä Í≥ÑÌöç,ÏóÖÏ≤¥ ÏÉÅÎã¥ ÏòàÏïΩ,Ï≤¥ÌÅ¨Î¶¨Ïä§Ìä∏ ÌôïÏù∏,ÌÉÄÏûÑÎùºÏù∏ Ï†êÍ≤Ä"
        )
        
        # Parse and validate suggestions
        suggestions = [s.strip() for s in suggestions_response.split(',')][:4]
        
        # Ensure minimum suggestions with smart defaults
        if len(suggestions) < 4:
            default_suggestions = ['ÏòàÏÇ∞ Í≥ÑÌöç', 'ÏóÖÏ≤¥ Ï∂îÏ≤ú', 'Ï§ÄÎπÑ Í∞ÄÏù¥Îìú', 'ÏùºÏ†ï Í¥ÄÎ¶¨']
            suggestions.extend(default_suggestions[:4-len(suggestions)])
        
        # Phase 6: UI-Optimized Quick Replies Generation
        quick_replies = []
        
        # Context-aware quick replies based on routing and results
        if routing_decision == 'tool_execution':
            if any('db_query' in str(r.get('tool_name', '')) for r in tool_results):
                quick_replies.extend(['Îçî ÎßéÏùÄ ÏóÖÏ≤¥', 'ÏÉÅÏÑ∏ Ï†ïÎ≥¥'])
            if any('calculator' in str(r.get('tool_name', '')) for r in tool_results):
                quick_replies.extend(['Îã§Î•∏ Í≥ÑÏÇ∞', 'ÏòàÏÇ∞ Ï°∞Ï†ï'])
        
        elif routing_decision == 'recommendation':
            quick_replies.extend(['Íµ¨Ï≤¥Ï†Å Ï∂îÏ≤ú', 'Îã®Í≥ÑÎ≥Ñ Í∞ÄÏù¥Îìú'])
        
        else:  # general_response
            if 'ÏòàÏÇ∞' in user_input:
                quick_replies.extend(['ÏòàÏÇ∞ Í≥ÑÏÇ∞Í∏∞', 'Ï†àÏïΩ ÌåÅ'])
            elif 'ÏóÖÏ≤¥' in user_input or 'Ï∂îÏ≤ú' in user_input:
                quick_replies.extend(['ÏóÖÏ≤¥ Ï∞æÍ∏∞', 'Î¶¨Î∑∞ ÌôïÏù∏'])
            else:
                quick_replies.extend(['ÎßûÏ∂§ Ï∂îÏ≤ú', 'Ï§ÄÎπÑ Í∞ÄÏù¥Îìú'])
        
        # Always include help option
        quick_replies.append('Îã§Î•∏ ÏßàÎ¨∏')
        quick_replies = quick_replies[:4]  # Limit for UI
        
        # Phase 7: Response Quality Assessment
        quality_check_prompt = f"""
        Evaluate this final response quality for wedding planning assistance:
        
        USER REQUEST: "{user_input}"
        GENERATED RESPONSE: {final_response_content[:300]}...
        USER CONTEXT: {personalization_string}
        
        Quality Assessment Criteria:
        1. Directly addresses user's specific request?
        2. Incorporates personal context appropriately? 
        3. Provides actionable next steps?
        4. Maintains encouraging, supportive tone?
        5. Information is accurate and helpful?
        
        Provide quality score and brief assessment (Korean, max 100 chars):
        """
        
        quality_assessment = safe_llm_invoke(
            quality_check_prompt,
            fallback_response="ÏùëÎãµ ÌíàÏßà ÏñëÌò∏ - Í∞úÏù∏ ÎßûÏ∂§ Ï†ïÎ≥¥ Ï†úÍ≥µ ÏôÑÎ£å"
        )
        
        # Phase 8: Final State Updates with Comprehensive Metadata
        state.update({
            'final_response': final_response_content,
            'suggestions': suggestions,
            'quick_replies': quick_replies,
            'response_metadata': {
                'generation_strategy': content_strategy,
                'personalization_context': personalization_string,
                'tool_insights_count': len(tool_insights),
                'quality_assessment': quality_assessment,
                'generation_timestamp': datetime.now().isoformat(),
                'word_count': len(final_response_content),
                'routing_path': routing_decision
            },
            'conversation_summary': f"ÏÇ¨Ïö©Ïûê ÏöîÏ≤≠ '{user_input}' Ï≤òÎ¶¨ ÏôÑÎ£å - {routing_decision} Í≤ΩÎ°ú ÌÜµÌï¥ Í∞úÏù∏ ÎßûÏ∂§ ÏùëÎãµ Ï†úÍ≥µ",
            'status': 'ok'
        })
        # ÏÉàÎ°ú Ï∂îÍ∞ÄÌïú Î∂ÄÎ∂Ñ(2Ï§Ñ)
        current_messages = state.get('messages', [])
        state['messages'] = current_messages + [AIMessage(content=final_response_content)]

        print(f"‚ú® Response Generated: {len(final_response_content)} chars")
        print(f"üéØ Quality: {quality_assessment}")
        print(f"üí° Suggestions: {len(suggestions)} provided")
        
    except Exception as e:
        print(f"üí• Response generation failed: {e}")
        
        # Intelligent fallback response generation
        fallback_response_prompt = f"""
        Create a helpful fallback response for this wedding planning request:
        
        User asked: "{user_input}"
        Available context: {personalization_string if 'personalization_string' in locals() else 'Limited context'}
        
        Generate encouraging, helpful response despite technical issues (Korean, max 400 chars):
        Focus on wedding planning guidance and next steps.
        """
        
        fallback_response = safe_llm_invoke(
            fallback_response_prompt,
            fallback_response=f"""Í≤∞Ìòº Ï§ÄÎπÑÏôÄ Í¥ÄÎ†®Îêú '{user_input}' ÏöîÏ≤≠Ïóê ÎåÄÌï¥ ÎèÑÏõÄÏùÑ ÎìúÎ¶¨Í≤†ÏäµÎãàÎã§.

ÌòÑÏû¨ ÏùºÏãúÏ†ÅÏù∏ Ï≤òÎ¶¨ ÏßÄÏó∞Ïù¥ ÏûàÏßÄÎßå, Í≤∞Ìòº Ï§ÄÎπÑÏùò ÌïµÏã¨Ï†ÅÏù∏ Î∂ÄÎ∂ÑÎì§ÏùÑ ÏïàÎÇ¥Ìï¥ ÎìúÎ¶¥ Ïàò ÏûàÏäµÎãàÎã§.

üìã **Í∏∞Î≥∏ Í≤∞Ìòº Ï§ÄÎπÑ Í∞ÄÏù¥Îìú:**
- ÏòàÏÇ∞ Í≥ÑÌöçÍ≥º Ïö∞ÏÑ†ÏàúÏúÑ ÏÑ§Ï†ï
- Ïõ®Îî©ÌôÄÍ≥º Ï£ºÏöî ÏóÖÏ≤¥ ÏòàÏïΩ
- Ï§ÄÎπÑ ÏùºÏ†ï Ï≤¥ÌÅ¨Î¶¨Ïä§Ìä∏ Í¥ÄÎ¶¨

Íµ¨Ï≤¥Ï†ÅÏù∏ ÏßàÎ¨∏Ïù¥ÎÇò ÎèÑÏõÄÏù¥ ÌïÑÏöîÌïú Î∂ÄÎ∂ÑÏù¥ ÏûàÏúºÏãúÎ©¥ Ïñ∏Ï†úÎì† ÎßêÏîÄÌï¥ Ï£ºÏÑ∏Ïöî!"""
        )
        
        state.update({
            'final_response': fallback_response,
            'suggestions': ['ÏòàÏÇ∞ Í≥ÑÌöç', 'ÏóÖÏ≤¥ Ï∂îÏ≤ú', 'Ï§ÄÎπÑ Ï≤¥ÌÅ¨Î¶¨Ïä§Ìä∏', 'ÏùºÏ†ï Í¥ÄÎ¶¨'],
            'quick_replies': ['ÏòàÏÇ∞', 'ÏóÖÏ≤¥', 'Ï≤¥ÌÅ¨Î¶¨Ïä§Ìä∏', 'ÎèÑÏõÄÎßê'],
            'status': 'ok',  # Graceful degradation
            'response_metadata': {
                'generation_type': 'fallback',
                'error_handled': str(e),
                'timestamp': datetime.now().isoformat()
            }
        })
        # 2Ï§Ñ Ï∂îÍ∞Ä: fallback ÏùëÎãµÎèÑ MessagesStateÏóê Ï∂îÍ∞Ä
        current_messages = state.get('messages', [])
        state['messages'] = current_messages + [AIMessage(content=fallback_response)]
    
    return state